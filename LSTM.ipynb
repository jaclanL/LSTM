{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 0\n",
    "        self.window_size = 0\n",
    "        self.hidden_layers = 0\n",
    "        self.clip_margin = 0\n",
    "        self.learning_rate = 0\n",
    "        self.epochs = 0\n",
    "    ############################################################################\n",
    "    # Defining hyperparameters, THEY SHOULD BE MODIFIED\n",
    "    batch_size = 10  # number of windows passed at once\n",
    "    window_size = 10  # number of data points we want to predict\n",
    "    hidden_layers = 250  # number of hidden layers in the LSTM\n",
    "    clip_margin = 4  # used to clip gradients above/below its margins\n",
    "    learning_rate = 0.0001  # way to optimize the loss function\n",
    "    epochs = 200  # number of iterations the model needs to make (forward and back propagation)\n",
    "\n",
    "    ############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Windowing function ####################################\n",
    "    def windowed_data(data, window_size):\n",
    "        X = []\n",
    "        y = []\n",
    "        i = 0\n",
    "\n",
    "        while (i + window_size) <= len(data) - 1:\n",
    "            X.append(data[i:i + window_size])\n",
    "            y.append(data[i + window_size])\n",
    "\n",
    "            i += 1\n",
    "        assert len(y) == len(X)\n",
    "        return X, y\n",
    "\n",
    "    ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Sigmoid function ######################################\n",
    "    def calcSig(inputs, outputs, wGate, wHidden, bias):\n",
    "        gate = tf.sigmoid(tf.matmul(wGate, inputs) + tf.matmul(wHidden, outputs) + bias)\n",
    "        return gate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## LSTM-block function ###################################\n",
    "    def block(inputs, outputs, cell_state, array_of_weights, array_of_bias): # hidden_layers\n",
    "        inputG = calcSig(inputs, outputs, array_of_weights[0], array_of_weights[1], array_of_bias[0])\n",
    "        forgetG = calcSig(inputs, outputs, array_of_weights[2], array_of_weights[3], array_of_bias[1])\n",
    "        outputG = calcSig(inputs, outputs, array_of_weights[4], array_of_weights[5], array_of_bias[2])\n",
    "        memoryG = calcSig(inputs, outputs, array_of_weights[6], array_of_weights[7], array_of_bias[3])\n",
    "\n",
    "        cell_state = cell_state * forgetG + inputG * memoryG    # final cell state C\n",
    "        output_f = tf.tanh(cell_state) * outputG                # final output h\n",
    "\n",
    "        return cell_state, output_f\n",
    "\n",
    "    ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Loading data and converting it into a .csv file\n",
    "    data_xls = pd.read_excel('Sarcos.xls', 'Sheet1', index_col=None)\n",
    "    # data_xls.to_csv('Sarcos_csv.csv', encoding='utf-8')\n",
    "\n",
    "    data1 = data_xls.iloc[:, 1]  # reading from index 1 in the csv file\n",
    "    # print(data1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data1 = scaler.fit_transform(data1.values.reshape(-1, 1))\n",
    "\n",
    "    ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and validation data\n",
    "\n",
    "    X, y = test.windowed_data(data1, window_size)\n",
    "    size_of_80p = int(len(scaled_data1) * 0.8)\n",
    "\n",
    "    X_train = np.array(X[:size_of_80p])  # we want 80 % to be training data\n",
    "    y_train = np.array(y[:size_of_80p])\n",
    "\n",
    "    X_test = np.array(X[size_of_80p:])\n",
    "    y_test = np.array(y[size_of_80p:])\n",
    "\n",
    "    print(\"X_train size: {}\".format(X_train.shape))\n",
    "    print(\"y_train size: {}\".format(y_train.shape))\n",
    "    print(\"X_test size: {}\".format(X_test.shape))\n",
    "    print(\"y_test size: {}\".format(X_test.shape))\n",
    "    ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # LSTM weights\n",
    "\n",
    "    generated_values1 = tf.truncated_normal([1,hidden_layers],stddev = 0.05)\n",
    "    generated_values2 = tf.truncated_normal([hidden_layers,hidden_layers], stddev = 0.05)\n",
    "    bias_values = tf.zeros([hidden_layers])\n",
    "\n",
    "    # Weights\n",
    "    W_inputG = tf.Variable(generated_values1)       #  weights input gate\n",
    "    W_inputH = tf.Variable(generated_values2)       #  weights input hidden layer\n",
    "    W_forgetG = tf.Variable(generated_values1)\n",
    "    W_forgetH = tf.Variable(generated_values2)\n",
    "    W_outputG = tf.Variable(generated_values1)\n",
    "    W_outputH = tf.Variable(generated_values2)\n",
    "    W_memoryG = tf.Variable(generated_values1)\n",
    "    W_memoryH = tf.Variable(generated_values2)\n",
    "    W_Output = tf.Variable(generated_values1)\n",
    "\n",
    "    array_of_weights = np.array([W_inputG, W_inputH, W_forgetG,W_forgetH,W_outputG,W_outputH, W_memoryG, W_memoryH,W_Output])\n",
    "     # Bias\n",
    "    B_input = tf.Variable(bias_values)\n",
    "    B_forget = tf.Variable(bias_values)\n",
    "    B_output = tf.Variable(bias_values)\n",
    "    B_memory = tf.Variable(bias_values)\n",
    "    B_Output = tf.Variable(tf.zeros(1))\n",
    "\n",
    "    array_of_bias = np.array([B_input, B_forget, B_output, B_memory, B_Output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "    inputs = tf.placeholder(tf.float32, [batch_size,window_size, 1])\n",
    "    targets = tf.placeholder(tf.float32, [batch_size, 1])\n",
    "\n",
    "    ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Loop ######################################\n",
    "    outputs_h =[]\n",
    "    for ii in range(batch_size):\n",
    "        batch_state = np.zeros([1, hidden_layers])\n",
    "        batch_output = np.zeros([1, hidden_layers])\n",
    "\n",
    "        for jj in range(window_size):\n",
    "            batch_state, batch_output = block(tf.reshape(inputs[ii][jj], (-1,1)), batch_output, batch_state, array_of_weights,array_of_bias)\n",
    "\n",
    "        outputs_h.append(tf.matmul(batch_output,W_Output) + B_Output)\n",
    "\n",
    "    outputs_h\n",
    "\n",
    "##################################################################################\n",
    "###### Looking at the performance by defining the loss by mean squared error #####\n",
    "    errors = []\n",
    "\n",
    "    for ii in range(len(outputs_h)):\n",
    "        aux_reshape = tf.reshape(targets[ii],(-1,1))\n",
    "        errors.append(tf.error.mean_squared_error(aux_reshape), outputs_h[ii])\n",
    "\n",
    "    error = tf.reduce_mean(errors)\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Training ###################################\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    trained_optimizer = 0; # change this\n",
    "    for ii in range(epochs):\n",
    "        trained_data = []\n",
    "        jj = 0\n",
    "        epoch_error = []\n",
    "\n",
    "        while(jj + batch_size) <= len(X_train):\n",
    "            X_batch = X_train[jj:jj + batch_size]\n",
    "            y_batch = y_train[jj:jj + batch_size]\n",
    "            a,b = session.run([outputs_h, error, trained_optimizer],feed_dict = {inputs:X_batch, targets:y_batch})\n",
    "            trained_data.append(a)\n",
    "            epoch_error.append(b)\n",
    "            jj += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Plotting the data ######################################\n",
    "\"\"\"plt.figure(figsize=(12,7),frameon=False, facecolor='brown',edgecolor='blue')\n",
    "plt.title('Data column 1')\n",
    "plt.xlabel('Time steps t')\n",
    "plt.ylabel('Position of joint 1')\n",
    "plt.plot(scaled_data1,label='pos,joint1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(16,7))\n",
    "    plt.title('Data')\n",
    "    plt.xlabel('Time steps t')\n",
    "    plt.ylabel('Position of joints')\n",
    "    plt.plot(scaled_data1,label='Original data')\n",
    "    plt.plot(tranied_data,label = 'Training data')\n",
    "    plt.plot(test_data, label = 'Testing data')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve by reducing the window size\n",
    "\n",
    "## To do:\n",
    "# * implement in Jupyter, make it work\n",
    "# *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
