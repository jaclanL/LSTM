{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ESN\n",
    "from scipy.sparse import rand as sprand\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigs as speigs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv data for input and output\n",
    "\n",
    "# 7 columns each for joint angles, angular velocities and angular acceleraions\n",
    "uVec = pd.read_csv(\"Sarcos.csv\", usecols=np.arange(21),  header=None,\n",
    "                  names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', \n",
    "                           'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7'])\n",
    "# 7 columns for joint torques\n",
    "yVec = pd.read_csv(\"Sarcos.csv\", usecols=np.arange(21, 28, 1), header=None, \n",
    "                  names = ['T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7'])\n",
    "\n",
    "# to numpy array\n",
    "u = uVec.values[:, :, np.newaxis]\n",
    "y = yVec.values[:, :, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Organized Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfOrganizedLayer:\n",
    "    '''''Self organized layer for Neural Networks using the Generalized Hebbian Learning (GHL) algorithm to update weights. \n",
    "    Functions:\n",
    "        * __init__ : initilizing the network layer, creating a random weight matrix and related variables\n",
    "        * updateOutput: calcualtes the output for a given input vector. The input is centerd within the function\n",
    "        * updateWeightMatrix: updates the weight accoring to the GHL algorithm'''''\n",
    "    \n",
    "    def __init__(self, layer_size, input_size, eta):\n",
    "        '''layer_size - number of neurons\n",
    "        input_size - number of inputs\n",
    "        eta - learning rate'''\n",
    "        \n",
    "        # initalize W with random weights\n",
    "        self.W = np.random.rand(size = (layer_size, input_size))*2-1\n",
    "        # self.W = self.W/np.sum(self.W)\n",
    "        \n",
    "        # save learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # initalize input sum and mean\n",
    "        self.uMean = 0 \n",
    "        self.uAbs = 0.00001\n",
    "        self.i = 0\n",
    "    \n",
    "    def updateOutput(self, u):\n",
    "        '''u - network input vector'''\n",
    "        \n",
    "        # normalize and center input\n",
    "        self.uMean = self.uMean + (u-self.uMean)/(self.i+1)\n",
    "        uCenter = u - self.uMean\n",
    "        self.uAbs = np.maximum(self.uAbs, np.absolute(uCenter))\n",
    "        uNormCenter = uCenter/self.uAbs\n",
    "        \n",
    "        # update index\n",
    "        self.i += 1\n",
    "        \n",
    "        # calculate output\n",
    "        s = np.tanh(self.W@uNormCenter)\n",
    "        \n",
    "        return (uNormCenter, s)\n",
    "    \n",
    "    def updateWeights(self, u, s):\n",
    "        '''u - network input vector\n",
    "        s - network output'''\n",
    "        # transpose\n",
    "        uT = np.transpose(u)\n",
    "        sT = np.transpose(s)\n",
    "        \n",
    "        # calculate GHL update \n",
    "        triang = np.tril(s@sT);\n",
    "        dW = self.eta*(s@uT - triang@self.W);\n",
    "        \n",
    "        # update W matrix\n",
    "        self.W += dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def computeRMSE():\n",
    "\n",
    "def normalize_input(inputSequence):\n",
    "    iMu = mean(inputSequence, axis=0)\n",
    "    iMax = amax(abs(inputSequence), axis=0)\n",
    "    normInputSequence = (inputSequence-iMu)/iMax\n",
    "    return normInputSequence\n",
    "\n",
    "class ESN():\n",
    "    def __init__(self,nInputUnits,nReservoirUnits,nOutputUnits,spectralRadius):\n",
    "        print('Creating Echo State Network...')\n",
    "        ### STRUCTURE\n",
    "        self.nInputUnits = nInputUnits\n",
    "        self.nReservoirUnits = nReservoirUnits\n",
    "        self.nOutputUnits = nOutputUnits\n",
    "        self.nTotalUnits = nInputUnits + nReservoirUnits + nOutputUnits\n",
    "        \n",
    "        self.spectralRadius = spectralRadius\n",
    "\n",
    "        ### INITIALIZE WEIGHTS\n",
    "        ## JAEGER (Sparse reservoir weights) (Polydoros et. al. Algorithm 1)\n",
    "        success = 0                                             \n",
    "        while success == 0: # following block might fail\n",
    "            try:\n",
    "                self.Wres = sprand(nReservoirUnits, nReservoirUnits, density=10/nReservoirUnits)\n",
    "                self.Wres = self.Wres.toarray()\n",
    "                self.Wres[self.Wres!=0] -= 0.5 # modify only nonzero elements\n",
    "                self.Wres = csr_matrix(self.Wres) # back to sparse\n",
    "                maxVal = max(abs(speigs(A=self.Wres, k=1, which='LM')[0]))\n",
    "                self.Wres /= maxVal\n",
    "                success = 1\n",
    "            except:\n",
    "                success = 0   \n",
    "        self.Wres *= self.spectralRadius\n",
    "        \n",
    "        ## MANTAS (Standard array)\n",
    "        #self.Wres = random.rand(nReservoirUnits, nReservoirUnits)-0.5\n",
    "        #rho = max(abs(scipy.linalg.eig(self.Wres)[0]))\n",
    "        #self.Wres *= 1.25 / rho\n",
    "\n",
    "        self.Win = 2.0 * random.rand(nReservoirUnits, nInputUnits)- 1.0\n",
    "        self.Wout = zeros((nOutputUnits, nReservoirUnits + nInputUnits))\n",
    "        self.Wfb = (2.0 * random.rand(nReservoirUnits, nOutputUnits)- 1.0)\n",
    "\n",
    "        ### INIT DEFAULT PARAMETERS\n",
    "        self.inputScaling = ones((nInputUnits, 1)) # MAKE SURE INPUT IS NORMALIZED!!!\n",
    "        self.inputShift = zeros((nInputUnits, 1))\n",
    "        self.teacherScaling = ones((nOutputUnits, 1)) # DOES TEACHER SCALING MAKE ANY DIFFERENCE???\n",
    "        self.teacherShift = zeros((nOutputUnits, 1))\n",
    "        self.teacherForcing = True # Desired output y_teacher instead of predicted y -> Ridge regression!!!  \n",
    "        self.feedbackScaling = zeros((nOutputUnits, 1))\n",
    "        \n",
    "        self.noiseLevel = 0.0\n",
    "        self.leakingRate = 1\n",
    "        self.forgetPoints = 100\n",
    "        self.reg = 1e-5 # If ridge regression!!! (something else than 1)\n",
    "        \n",
    "        self.RLS_lambda = 0.9999995\n",
    "        self.RLS_delta = 0.000001\n",
    "\n",
    "        self.trained = 0\n",
    "        self.pseudo = True\n",
    "        \n",
    "        print('Successful!')\n",
    "    \n",
    "    def train_single(self,inputSequence,outputSequence):\n",
    "        ## STATE COLLECTION\n",
    "        nDataPoints = outputSequence.shape[0]\n",
    "        self.stateCollect = zeros((nDataPoints - self.forgetPoints, self.nInputUnits + self.nReservoirUnits))\n",
    "        self.totalState = zeros((self.nInputUnits + self.nReservoirUnits + self.nOutputUnits, 1))\n",
    "        self.reservoirState = zeros((self.nReservoirUnits, 1))\n",
    "        \n",
    "        print('Training...')\n",
    "        collectIndex = 0;\n",
    "        for i in range(nDataPoints):\n",
    "            IN = self.inputScaling * array([inputSequence[i,:]]).T + self.inputShift\n",
    "            self.totalState[self.nReservoirUnits:self.nReservoirUnits+self.nInputUnits, :] = IN;\n",
    "\n",
    "            # PLAIN ESN (OR LEAKY) # Wres.array if sparse\n",
    "            self.reservoirState = tanh(hstack((self.Wres.toarray(), self.Win, self.Wfb@ \\\n",
    "                                               diag(self.feedbackScaling[:,0])))@self.totalState)\n",
    "        \n",
    "            # Adding noise, more computational but seems to stabilize solutions in models with output feedback\n",
    "            self.reservoirState += self.noiseLevel *(random.rand(self.nReservoirUnits,1) - 0.5)\n",
    "            \n",
    "            if self.teacherForcing:\n",
    "                self.netOut = self.teacherScaling * array([outputSequence[i,:]]).T + self.teacherShift\n",
    "            else:\n",
    "                self.netOut = self.Wout @ vstack((self.reservoirState, IN)) # activation function identity\n",
    "                \n",
    "            self.totalState = vstack((self.reservoirState, IN, self.netOut))\n",
    "            \n",
    "            if i > self.forgetPoints:\n",
    "                collectIndex = collectIndex + 1;\n",
    "                self.stateCollect[collectIndex,:] = hstack((self.reservoirState.T, IN.T))\n",
    "        \n",
    "        ## TEACHER COLLECTION\n",
    "        nOutputPoints  = outputSequence.shape[0]\n",
    "        self.teacherCollect = zeros(((nOutputPoints - self.forgetPoints), self.nOutputUnits))\n",
    "        \n",
    "        outputSequence = outputSequence[self.forgetPoints:,:]\n",
    "        nOutputPoints = outputSequence.shape[0] # update the size of outputSequence\n",
    "\n",
    "        self.teacherCollect = array((diag(self.teacherScaling[:,0]) @ outputSequence.T).T + \\\n",
    "                                    matlib.repmat(self.teacherShift.T,nOutputPoints, 1))\n",
    "        \n",
    "        if self.pseudo: # (WIENER-HOPF, faster but less stable than pin, H.Jaeger)\n",
    "            self.Wout = (linalg.pinv(self.stateCollect)@self.teacherCollect).T\n",
    "                               \n",
    "        else: # Ridge regression 'Tikhonov' (the larger alpha the smoother output,zero then same)\n",
    "            covMat = self.stateCollect.T @ self.stateCollect / self.stateCollect.shape[0]\n",
    "            pVec = self.stateCollect.T @ self.teacherCollect / self.stateCollect.shape[0]\n",
    "            self.Wout = (linalg.inv(covMat+(self.reg**2)*eye(covMat.shape[0])) @ pVec).T\n",
    "        \n",
    "        self.trained = 1\n",
    "        print('Training finished!')\n",
    "        \n",
    "        \n",
    "    def train_online(self,trainInput,trainOutput): # FROM H.JAEGER TOOLBOX (RLS ALGORITHM)\n",
    "        nSampleInput = trainInput.shape[0]\n",
    "        self.stateCollect = zeros((nSampleInput, self.nReservoirUnits + self.nInputUnits))\n",
    "        SInverse = 1 / self.RLS_delta * eye(self.nReservoirUnits + self.nInputUnits)\n",
    "        self.totalState = zeros((self.nTotalUnits,1))\n",
    "        reservoirState = zeros((self.nReservoirUnits,1)) \n",
    "        error = zeros((nSampleInput , 1)) \n",
    "        weights = zeros((nSampleInput , 1)) \n",
    "        \n",
    "        print('Training...')\n",
    "        for j in range(nSampleInput):\n",
    "            IN = self.inputScaling * array([trainInput[j,:]]).T + self.inputShift \n",
    "            \n",
    "            #write input into totalstate\n",
    "            self.totalState[self.nReservoirUnits:self.nReservoirUnits+self.nInputUnits,:] = IN\n",
    "            \n",
    "            # update totalstate except at input positions\n",
    "            self.reservoirState = tanh(hstack((self.Wres.todense(), self.Win, self.Wfb@ \\\n",
    "                                               diag(self.feedbackScaling[:,0])))@self.totalState)\n",
    "        \n",
    "            # Adding noise, more computational but seems to stabilize solutions in models with output feedback\n",
    "            self.reservoirState += self.noiseLevel *(random.rand(self.nReservoirUnits,1) - 0.5)\n",
    "            \n",
    "            self.netOut = self.Wout @ vstack((self.reservoirState, IN)) # activation function identity\n",
    "            self.totalState = vstack((self.reservoirState, IN, self.netOut))  \n",
    "            \n",
    "            state = vstack((self.reservoirState, IN)) \n",
    "            \n",
    "            self.stateCollect[j, :] = state.T\n",
    "            phi = state.T * SInverse\n",
    "            k = phi.T/(self.RLS_lambda + phi * state)\n",
    "            e = self.teacherScaling * trainOutput[j,0] + self.teacherShift - self.netOut[0,:]\n",
    "            \n",
    "            # update the weights \n",
    "            self.Wout[0,:] = self.Wout[0,:] + (k*e).T \n",
    "                                               \n",
    "            SInverse = (SInverse-k*phi) / self.RLS_lambda\n",
    "            \n",
    "        self.trained = 1\n",
    "        print('Training finished!')\n",
    "        \n",
    "                               \n",
    "    def test(self, inputSequence):\n",
    "        nDataPoints = inputSequence.shape[0]\n",
    "        self.stateCollect = zeros((nDataPoints - self.forgetPoints, self.nInputUnits + self.nReservoirUnits))\n",
    "        self.totalState = zeros((self.nInputUnits + self.nReservoirUnits + self.nOutputUnits, 1))\n",
    "        self.reservoirState = zeros((self.nReservoirUnits, 1))\n",
    "        \n",
    "        collectIndex = 0;\n",
    "        for i in range(nDataPoints):\n",
    "            IN = self.inputScaling * array([inputSequence[i,:]]).T + self.inputShift\n",
    "            self.totalState[self.nReservoirUnits:self.nReservoirUnits+self.nInputUnits, :] = IN;\n",
    "\n",
    "            # Plain update\n",
    "            self.reservoirState = tanh(hstack((self.Wres.toarray(), self.Win, self.Wfb@ \\\n",
    "                                               diag(self.feedbackScaling[:,0])))@self.totalState)\n",
    "        \n",
    "            # Adding noise, more computational but seems to stabilize solutions in models with output feedback\n",
    "            self.reservoirState += self.noiseLevel *(random.rand(self.nReservoirUnits,1) - 0.5)\n",
    "            \n",
    "            self.netOut = self.Wout @ vstack((self.reservoirState, IN)) # activation function identity\n",
    "                \n",
    "            self.totalState = vstack((self.reservoirState, IN, self.netOut))\n",
    "            \n",
    "            if i > self.forgetPoints:\n",
    "                collectIndex = collectIndex + 1;\n",
    "                self.stateCollect[collectIndex,:] = hstack((self.reservoirState.T, IN.T))\n",
    "                \n",
    "        yPred = self.stateCollect @ self.Wout.T\n",
    "        \n",
    "        # plot prediction vs true sequence\n",
    "        plt.figure(2)\n",
    "        plt.plot(yPred[0:200,0], 'c--',label='Predicted')\n",
    "        plt.xlabel('Sample')\n",
    "        plt.ylabel('Torque [Nm]')\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class output_layer:\n",
    "    def __init__(self, s_size, r_size, n_output, sigma_2 = 0.1, phi_2 = 1):\n",
    "        \n",
    "        # save parameters\n",
    "        self.sigma_2 = sigma_2\n",
    "        self.phi_2 = phi_2\n",
    "        \n",
    "        # init weights\n",
    "        self.W_train = np.zeros((n_output, s_size + r_size))\n",
    "        \n",
    "        #Initialize V\n",
    "        self.V = self.sigma_2 * np.identity(s_size + r_size)\n",
    "        \n",
    "    def updateWeights(self, c_t, tau):\n",
    "        # save previous V\n",
    "        V_prev = self.V\n",
    "        \n",
    "        self.V = np.linalg.inv(self.V + (1/self.sigma_2)* c_t @ c_t.T)\n",
    "        \n",
    "        a = self.V @ np.linalg.inv(V_prev) @ self.W_train.T\n",
    "        b = 1/self.sigma_2 * self.V @ c_t @ tau.T\n",
    "        print(b.shape)\n",
    "        print(a.shape)\n",
    "        self.W_train = np.sum([a.T, b.T], axis=0)\n",
    "            \n",
    "    def updateOutput(self, c_t):\n",
    "        o_dot = np.dot(self.W_train, c_t)\n",
    "        return o_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer = output_layer(21, 100, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 7)\n",
      "(121, 7)\n",
      "[[0.99991736]\n",
      " [0.99991736]\n",
      " [0.99991736]\n",
      " [0.99991736]\n",
      " [0.99991736]\n",
      " [0.99991736]\n",
      " [0.99991736]]\n"
     ]
    }
   ],
   "source": [
    "c_t = np.ones((21+100, 1))\n",
    "tau = np.ones((7,1))\n",
    "o = Layer.updateOutput(c_t)\n",
    "Layer.updateWeights(c_t, tau)\n",
    "Layer.W_train\n",
    "print(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
